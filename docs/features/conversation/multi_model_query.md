# 多模型询问

当面对一个复杂问题时，不同的大语言模型可能会给出风格各异的回答。GPT Academic 的多模型询问功能允许您将同一问题同时发送给多个模型，并在同一界面中并排查看它们的回答。这不仅能帮助您获得更全面的视角，还能直观比较不同模型的能力差异。

---

## 功能价值

为什么要同时询问多个模型？这个功能在以下场景中特别有用：

**验证答案可靠性**。当您对某个问题的答案不确定时，多个模型给出一致的回答可以增强可信度；如果回答存在分歧，则提示您需要进一步核实。这种"交叉验证"的方式在处理事实性问题时尤为重要。

**比较模型特点**。不同模型有各自的优势领域。通过同时询问，您可以直观感受哪个模型更适合您的特定需求——也许 GPT-4 在推理方面更强，而通义千问的中文表达更地道。

**获取多元观点**。对于开放性问题，不同模型可能从不同角度给出回答，这些观点的综合能带来更丰富的启发。

---

## 使用方法

GPT Academic 提供两种多模型询问方式：使用预设模型列表的**快速模式**，以及临时指定模型的**自定义模式**。

### 快速模式

快速模式使用配置文件中预设的模型组合，操作最为简便。

首先，在输入框中输入您的问题。然后在函数插件区的**对话**分类中，找到并点击**询问多个GPT模型**按钮。系统会立即将您的问题发送给预设的多个模型，并在对话区实时显示各模型的回答。

回答以不同颜色区分显示，格式如下：

```
【gpt-3.5-turbo 说】: 这是 GPT-3.5 的回答...

---

【chatglm3 说】: 这是 ChatGLM3 的回答...
```

每个模型的回答都带有明确的标识，方便您进行比较和参考。

### 自定义模式

如果您希望临时指定要询问的模型，而不是使用预设列表，可以使用自定义模式。

在函数插件区找到**询问多个GPT模型（手动指定询问哪些模型）**插件。点击后会弹出高级参数输入区，在这里输入您想要询问的模型名称，多个模型之间用 `&` 符号连接：

```
gpt-4o&qwen-max&deepseek-chat
```

然后点击执行，系统会将问题发送给您指定的这三个模型。这种方式的灵活性更高，您可以根据当前任务的需要随时调整模型组合。

!!! tip "模型名称"
    模型名称必须与配置文件中 `AVAIL_LLM_MODELS` 列表里的名称完全一致。如果指定的模型未配置对应的 API KEY，该模型的请求会失败。

---

## 配置预设模型

快速模式使用的模型列表由 `MULTI_QUERY_LLM_MODELS` 配置项控制。您可以在 `config_private.py` 中修改这个设置：

```python
# 定义"询问多个GPT模型"插件使用的模型组合
# 多个模型之间用 & 符号分隔
MULTI_QUERY_LLM_MODELS = "gpt-4o&qwen-max&deepseek-chat"
```

配置时有几点需要注意。首先，列表中的每个模型都需要在 `AVAIL_LLM_MODELS` 中定义，并配置相应的 API KEY。其次，模型数量不宜过多，2-4 个是比较合理的范围——太多模型会导致界面拥挤，也会增加 API 调用成本。最后，建议选择响应速度相近的模型，否则快的模型等待慢的模型会影响整体体验。

以下是一些实用的模型组合示例：

**中英文能力对比**
```python
MULTI_QUERY_LLM_MODELS = "gpt-4o&qwen-max"
```

**推理能力对比**
```python
MULTI_QUERY_LLM_MODELS = "gpt-4o&deepseek-reasoner"
```

**多家国产模型对比**
```python
MULTI_QUERY_LLM_MODELS = "qwen-max&glm-4&deepseek-chat"
```

**快速响应 vs 深度思考**
```python
MULTI_QUERY_LLM_MODELS = "gpt-3.5-turbo&gpt-4o"
```

---

## 工作原理

理解多模型询问的工作原理，有助于您更好地使用这个功能。

当您触发多模型询问时，系统会为每个模型创建独立的请求线程。这些线程并行执行，同时向各自的模型发送请求。一个后台监控线程负责收集各线程的响应，并将结果汇总到界面上。

由于采用并行请求的方式，总等待时间取决于最慢的那个模型，而不是所有模型响应时间的总和。例如，如果 GPT-4 需要 10 秒响应，而 GPT-3.5 只需要 2 秒，那么整体等待时间约为 10 秒，而不是 12 秒。

在响应过程中，界面会实时更新各模型的输出。先完成的模型会先显示结果，您无需等待所有模型都完成就能开始阅读。

---

## 使用建议

为了获得最佳的使用体验，这里有一些实践建议：

**选择互补的模型**。与其选择能力相似的模型，不如选择各有专长的模型组合。例如，一个擅长中文、一个擅长代码、一个擅长推理，这样能获得更多元的视角。

**控制问题长度**。多模型询问会产生成倍的 API 调用量。对于较长的输入，建议先在单模型下调试，确认问题表述准确后再进行多模型比较。

**关注分歧点**。当多个模型给出相似的答案时，可以对结果更有信心；当答案出现明显分歧时，这些分歧点往往值得深入研究。

**结合历史记录**。多模型询问会清空对话历史以避免上下文过长。如果您需要基于之前的对话进行多模型比较，建议将相关上下文包含在问题中。

---

## 常见问题

??? question "为什么某个模型没有返回结果？"
    这通常是因为该模型的 API KEY 未配置或配置错误。请检查 `config_private.py` 中是否正确设置了对应的 API KEY。另外，网络问题或服务商限流也可能导致单个模型请求失败。

??? question "可以同时询问本地模型和在线模型吗？"
    可以。只要模型名称在 `AVAIL_LLM_MODELS` 中定义，并且相应的服务（本地模型服务或 API 服务）正常运行，就可以混合使用。例如：
    ```python
    MULTI_QUERY_LLM_MODELS = "chatglm4&gpt-4o"
    ```
    但请注意，本地模型的响应速度通常比在线 API 慢。

??? question "多模型询问会增加多少成本？"
    成本与询问的模型数量成正比。如果您询问 3 个模型，每次调用的 token 消耗就是原来的 3 倍。建议在需要比较时使用此功能，日常对话仍使用单模型。

??? question "回答显示的颜色可以自定义吗？"
    目前颜色是系统预设的，会按顺序为不同模型分配不同颜色。暂不支持自定义颜色设置。

---

## 相关文档

- [模型支持概览](../../models/overview.md) — 了解各模型的特点和配置方法
- [配置详解](../../get_started/configuration.md) — 完整的配置项说明
- [基础操作](../basic_operations.md) — 界面基本使用方法


