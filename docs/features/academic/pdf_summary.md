# 批量总结 PDF 文档

当您需要快速了解多篇论文的核心内容时，逐篇阅读显然效率太低。GPT Academic 的批量 PDF 总结功能可以自动为每篇论文生成结构化摘要，包括论文标题、作者信息、研究背景、方法和主要结论，让您在短时间内掌握大量文献的核心要点。

这项功能特别适合在进行文献综述、调研新领域或筛选相关论文时使用。相比于 [PDF 论文翻译](pdf_translation.md) 功能关注的是内容转换，本功能侧重于信息提炼和快速概览。

---

## 功能特点

批量 PDF 总结功能的设计目标是帮助您高效处理大量学术文献：

- **批量处理**：支持同时处理文件夹中的多个 PDF 文件，无需逐一操作
- **结构化输出**：生成标准化的总结格式，包含标题、作者、关键词、研究方法等关键信息
- **智能切分**：自动将长文档分割成适当片段，逐段提取核心内容后再整合
- **结果保存**：总结结果自动保存为文件，方便后续查阅和引用

---

## 前置条件

在使用批量 PDF 总结之前，请确保系统已安装必要的依赖。

!!! warning "依赖安装"
    本功能需要 `pymupdf` 库来解析 PDF 文件。如果您在使用时遇到依赖缺失的提示，请执行以下命令：
    
    ```bash
    pip install --upgrade pymupdf
    ```

此外，您还需要确保已配置可用的大语言模型 API。由于批量总结可能涉及多个文件，建议选用性价比较高的模型（如 `gpt-3.5-turbo` 或 `qwen-turbo`）以控制 API 费用。

---

## 使用方法

### 准备文件

首先，您需要准备要总结的 PDF 文件。本功能支持两种输入方式：

| 方式 | 操作说明 |
|-----|---------|
| 上传文件 | 将单个或多个 PDF 文件直接拖入对话区的文件上传区域 |
| 上传压缩包 | 将包含多个 PDF 的 `.zip` 压缩包拖入上传区域 |
| 指定路径 | 在输入框中填写本地文件夹路径（适合处理大量本地文件） |

如果选择上传方式，系统会自动将文件解压到临时目录并递归搜索其中所有的 `.pdf` 文件。

### 执行总结

完成文件准备后，按以下步骤启动总结任务：

1. 在函数插件下拉菜单中找到 **学术** 分类
2. 选择 **批量总结PDF文档** 插件
3. 点击执行按钮开始处理

<!-- IMAGE: feat_pdf_summary_01_workflow.png -->
<!-- 描述: 批量总结PDF的完整操作流程，展示主界面 -->
<!-- 标注: ① 文件上传区域（显示已上传的PDF文件）② 函数插件下拉菜单位置 ③ "批量总结PDF文档"插件选项 -->
<!-- 尺寸建议: 1000px -->
![批量总结PDF操作流程](../../images/feat_pdf_summary_01_workflow.png)

### 处理过程

系统会依次处理每个 PDF 文件。对于每篇论文，处理流程如下：

1. **文本提取**：使用 PyMuPDF 解析 PDF 内容，提取全部文本
2. **智能切分**：将长文档按 2500 Token 左右切割成多个片段
3. **元信息提取**：从首页提取论文标题、作者等元数据
4. **逐段总结**：对每个片段进行内容概括，同时保持上下文连贯
5. **最终整合**：综合所有片段的总结，生成完整的结构化报告

处理进度会实时显示在对话区，您可以看到当前正在处理哪个文件的哪个片段。对于较长的论文（超过 20 个片段），系统会给出提示，此类文档可能无法达到最佳效果。

---

## 输出结果

每篇论文的总结将以标准化格式呈现，包含以下内容：

| 项目 | 说明 |
|-----|------|
| **Title** | 论文标题（含中文翻译） |
| **Authors** | 所有作者姓名 |
| **Affiliation** | 第一作者所属机构 |
| **Keywords** | 论文关键词 |
| **URLs** | 论文链接和 GitHub 代码链接（如有） |
| **Summary** | 结构化摘要，涵盖研究背景、现有方法问题、本文方法、实验结果 |

总结完成后，结果会保存到文件中，您可以在界面右侧的**文件下载区**找到并下载。总结文件命名格式为 `总结_{时间戳}.md`。

!!! tip "结果使用建议"
    生成的总结采用 Markdown 格式，您可以直接复制到笔记软件中，或作为文献综述的初稿素材。结构化的格式也便于后续整理和比较多篇论文的异同。

---

## 使用技巧

**选择合适的模型**：批量总结会产生较多 API 调用。如果您处理的文件较多，建议使用 `gpt-3.5-turbo` 或 `qwen-turbo` 等性价比高的模型。如果对总结质量要求较高且文件数量有限，可以选用 `gpt-4o` 或 `qwen-max`。

**控制文件数量**：虽然功能支持批量处理，但一次处理过多文件可能导致等待时间过长。建议每批控制在 10 篇以内，既能保证处理效率，也便于及时查看结果。

**扫描版 PDF 的处理**：本功能依赖文本提取，对于扫描版 PDF（图片格式）无法直接处理。如果您的 PDF 是扫描件，建议先使用 OCR 工具转换为可检索的 PDF。

---

## 常见问题

???+ question "总结结果不够准确或信息有遗漏"
    这通常发生在论文较长或结构复杂的情况下。您可以：
    
    1. 尝试使用更强大的模型（如 `gpt-4o`）重新处理
    2. 对于关键论文，结合 [PDF 问答](pdf_qa.md) 功能进行深入交互
    3. 检查原 PDF 是否为扫描版，文本是否可正常提取

???+ question "处理速度很慢"
    批量总结需要对每篇论文进行多次 API 调用，处理时间与以下因素相关：
    
    - **论文长度**：一篇 20 页的论文可能需要 10+ 次 API 调用
    - **模型响应速度**：不同模型的响应时间差异较大
    - **并发设置**：可以在配置中调整 `DEFAULT_WORKER_NUM` 增加并行度
    
    建议在处理大量文件时选择响应更快的模型。

???+ question "部分 PDF 无法解析"
    可能的原因包括：
    
    1. **PDF 加密或有密码保护**：需要先解除保护
    2. **PDF 损坏**：尝试用 PDF 阅读器打开确认文件完整性
    3. **纯图片 PDF**：扫描版文档需要先 OCR 处理

---

## 相关文档

- [PDF 论文翻译](pdf_translation.md) — 将 PDF 论文完整翻译为中文
- [PDF 问答](pdf_qa.md) — 与单篇 PDF 进行深度问答交互
- [基础操作](../basic_operations.md) — 了解文件上传等基础操作


